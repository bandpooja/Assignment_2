{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Assignment-2\n",
        "### Pooja Bandal\n",
        "### 2022-03-08\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Cloning the repository and setting up the notebook for import"
      ],
      "metadata": {
        "id": "uaJl2fiNIyKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqmLxD0mF5Wr",
        "outputId": "fee65154-6d1f-409c-84cf-8c2ec286813a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Assignment_2'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 28 (delta 3), reused 24 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/Assignment_2\n",
        "!git clone https://github.com/bandpooja/Assignment_2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSpMVzZOGdV7",
        "outputId": "efbcf494-27bd-4af2-be44-ef3eaee8f224"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'Assignment_2', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "os.listdir('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNde3kQpGo26"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/content/Assignment_2')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I need to install nltk and pytrec-eval-terrier libraries again in the notebook because I used a different environment.\n"
      ],
      "metadata": {
        "id": "HMX7rdfJJHkT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhvcGB1HG6FN",
        "outputId": "a5fada4d-8496-4065-974e-bd72efb17572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytrec-eval-terrier\n",
            "  Downloading pytrec_eval_terrier-0.5.2-cp37-cp37m-manylinux2010_x86_64.whl (287 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 92 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 287 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pytrec-eval-terrier\n",
            "Successfully installed pytrec-eval-terrier-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytrec-eval-terrier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Importing modules\n",
        "\n",
        "Importing all the modules used in the project."
      ],
      "metadata": {
        "id": "PSleDkZ8JVHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MetQn-lIGxES"
      },
      "outputs": [],
      "source": [
        "import concurrent\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.preprocess import cleansing, cleansing_test, preprocess_pipeline\n",
        "from utils.ngram_model import NGramModel\n",
        "from utils.helper import load_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting and saving the models\n",
        "\n",
        "Saving and fitting the models."
      ],
      "metadata": {
        "id": "beYVtlocJe-o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRqbkzz_HrBd",
        "outputId": "10af744c-5b73-40bf-d8dc-0bfe2677d80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# region Model Fitting\n",
        "new_list = brown.sents()\n",
        "min_freq = 15\n",
        "tokenized_sent = preprocess_pipeline(new_list)\n",
        "final_train, vocabulary = cleansing(tokenized_sent, min_freq)\n",
        "\n",
        "ns = [1, 2, 3, 5, 10]\n",
        "model_loc = 'models'\n",
        "\n",
        "for n in ns:\n",
        "    model = NGramModel(n=n, model_loc=model_loc, vocabulary=vocabulary)\n",
        "    model.fit(final_train)\n",
        "    model.save_model()\n",
        "# endregion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validating the model\n",
        "\n",
        "Checking if it performs in a test case."
      ],
      "metadata": {
        "id": "ZjmkLUX7Jl-m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bj6mfEEIMAm",
        "outputId": "7d229dda-0255-4baa-b85f-e1ab88b433bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-gram model prediction: {1: [('<unk>', 0.09079413723563301)], 5: [('<unk>', 0.09079413723563301), ('the', 0.05431409351795359), (',', 0.045282573629980144), ('<s>', 0.04450944137313257), ('<e>', 0.04450944137313257)], 10: [('<unk>', 0.09079413723563301), ('the', 0.05431409351795359), (',', 0.045282573629980144), ('<s>', 0.04450944137313257), ('<e>', 0.04450944137313257), ('.', 0.039005919584930444), ('of', 0.028264349132865418), ('and', 0.02239677209520394), ('to', 0.020304812825922596), ('a', 0.018016465543606676)]}\n",
            "2-gram model prediction: {1: [('.', 0.0015676438313215238)], 5: [('.', 0.0015676438313215238), (',', 0.0015676438313215238), ('said', 0.0014108794481893713), ('<unk>', 0.0009405862987929142), ('of', 0.0007838219156607619)], 10: [('.', 0.0015676438313215238), (',', 0.0015676438313215238), ('said', 0.0014108794481893713), ('<unk>', 0.0009405862987929142), ('of', 0.0007838219156607619), ('that', 0.0007838219156607619), ('and', 0.0004702931493964571), ('was', 0.0004702931493964571), ('did', 0.0004702931493964571), ('recommended', 0.0004702931493964571)]}\n",
            "3-gram model prediction: {1: [('said', 0.001260239445494644)], 5: [('said', 0.001260239445494644), (',', 0.0011027095148078135), ('that', 0.0007876496534341525), ('.', 0.0004725897920604915), ('recommended', 0.0004725897920604915)], 10: [('said', 0.001260239445494644), (',', 0.0011027095148078135), ('that', 0.0007876496534341525), ('.', 0.0004725897920604915), ('recommended', 0.0004725897920604915), ('panel', 0.0004725897920604915), ('<unk>', 0.0004725897920604915), ('further', 0.000315059861373661), ('charge', 0.000315059861373661), ('and', 0.000315059861373661)]}\n",
            "5-gram model prediction: {1: [('said', 0.0004745333755140778)], 5: [('said', 0.0004745333755140778), ('<unk>', 0.0004745333755140778), ('further', 0.00031635558367605187), (',', 0.00031635558367605187), ('did', 0.00031635558367605187)], 10: [('said', 0.0004745333755140778), ('<unk>', 0.0004745333755140778), ('further', 0.00031635558367605187), (',', 0.00031635558367605187), ('did', 0.00031635558367605187), ('also', 0.00031635558367605187), ('asked', 0.00031635558367605187), ('wanted', 0.00031635558367605187), ('the', 0.00015817779183802593), ('fulton', 0.00015817779183802593)]}\n",
            "10-gram model prediction: {1: [('said', 0.0004745333755140778)], 5: [('said', 0.0004745333755140778), ('<unk>', 0.0004745333755140778), ('further', 0.00031635558367605187), (',', 0.00031635558367605187), ('did', 0.00031635558367605187)], 10: [('said', 0.0004745333755140778), ('<unk>', 0.0004745333755140778), ('further', 0.00031635558367605187), (',', 0.00031635558367605187), ('did', 0.00031635558367605187), ('also', 0.00031635558367605187), ('asked', 0.00031635558367605187), ('wanted', 0.00031635558367605187), ('the', 0.00015817779183802593), ('fulton', 0.00015817779183802593)]}\n"
          ]
        }
      ],
      "source": [
        "# region validation\n",
        "previous_tokens = [\"the\", \"jury\"]\n",
        "\n",
        "for n in ns:\n",
        "    model = NGramModel(n=n, model_loc=model_loc, vocabulary=vocabulary)\n",
        "    model.load_model()\n",
        "    print(f\"{n}-gram model prediction: {model.get_suggestions(previous_tokens)}\")\n",
        "# endregion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "Making predictions on all the data in `APPLING1DAT.643` file."
      ],
      "metadata": {
        "id": "I4pSmyz5J1u6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kudi1vVjH9MQ",
        "outputId": "4d5364a8-85ff-4e92-d3fa-b0b6658d98f0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predictions-1-gram-model: 100%|██████████| 198/198 [00:00<00:00, 44805.62it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################\n",
            "Stats of 1-gram model\n",
            "####################\n",
            "success_1 average: 0.0\n",
            "success_10 average: 0.0\n",
            "success_5 average: 0.0\n",
            "####################\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predictions-2-gram-model: 100%|██████████| 198/198 [00:00<00:00, 25089.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################\n",
            "Stats of 2-gram model\n",
            "####################\n",
            "success_1 average: 0.007407407407407408\n",
            "success_10 average: 0.007407407407407408\n",
            "success_5 average: 0.007407407407407408\n",
            "####################\n"
          ]
        },
                {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predictions-3-gram-model: 100%|██████████| 198/198 [00:00<00:00, 25089.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################\n",
            "Stats of 3-gram model\n",
            "####################\n",
            "success_1 average: 0.002222222222222223\n",
            "success_10 average: 0.002222222222222223\n",
            "success_5 average: 0.002222222222222223\n",
            "####################\n"
          ]
        },
                {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predictions-5-gram-model: 100%|██████████| 198/198 [00:00<00:00, 25089.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################\n",
            "Stats of 5-gram model\n",
            "####################\n",
            "success_1 average: 0.007407407407407408\n",
            "success_10 average: 0.014814814814814815\n",
            "success_5 average: 0.007407407407407408\n",
            "####################\n"
          ]
        },
                {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predictions-10-gram-model: 100%|██████████| 198/198 [00:00<00:00, 25089.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################\n",
            "Stats of 10-gram model\n",
            "####################\n",
            "success_1 average: 0.007407407407407408\n",
            "success_10 average: 0.014814814814814815\n",
            "success_5 average: 0.007407407407407408\n",
            "####################\n"
          ]
        }
      ],
      "source": [
        "# region testing\n",
        "# Making prediction on all the birdbeck data Just checking success at k for all the predictions\n",
        "test_df = load_test(file_loc='/content/Assignment_2/data/APPLING1DAT.643')\n",
        "tokenized_sent = preprocess_pipeline(test_df['previous-tokens'].values.tolist(), remove_empty=False)\n",
        "final_test = cleansing_test(tokenized_sent, vocabulary)\n",
        "test_df['final-test'] = final_test\n",
        "ns = [1,2]\n",
        "queries = [{} for _ in ns]\n",
        "results_eval = [{} for _ in ns]\n",
        "\n",
        "for idx, n in enumerate(ns):\n",
        "    model = NGramModel(n=n, model_loc=model_loc, vocabulary=vocabulary)\n",
        "    model.load_model()\n",
        "\n",
        "    argument_list = final_test\n",
        "    suggestions = []\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        for result in executor.map(model.get_suggestions, argument_list):\n",
        "            suggestions.append(result)\n",
        "\n",
        "    query = queries[idx]\n",
        "    result_eval = results_eval[idx]\n",
        "    for fill_in_word, test_previous_tokens, suggestion in tqdm(zip(test_df['fill-in-word'], final_test,\n",
        "                                                                    suggestions), total=len(final_test),\n",
        "                                                                desc=f'Predictions-{n}-gram-model'):\n",
        "        query[f\"{' '.join(test_previous_tokens)} *\"] = {fill_in_word: 1}\n",
        "        result_eval[f\"{' '.join(test_previous_tokens)} *\"] = {}\n",
        "\n",
        "        for word in [w[0] for w in suggestion[1]]:\n",
        "            result_eval[f\"{' '.join(test_previous_tokens)} *\"][word] = 1\n",
        "\n",
        "        for word in [w[0] for w in suggestion[5]]:\n",
        "            if word not in result_eval[f\"{' '.join(test_previous_tokens)} *\"].keys():\n",
        "                result_eval[f\"{' '.join(test_previous_tokens)} *\"][word] = 1 / 5\n",
        "\n",
        "        for word in [w[0] for w in suggestion[10]]:\n",
        "            if word not in result_eval[f\"{' '.join(test_previous_tokens)} *\"].keys():\n",
        "                result_eval[f\"{' '.join(test_previous_tokens)} *\"][word] = 1 / 10\n",
        "\n",
        "    print('#' * 20)\n",
        "    print(f'Stats of {n}-gram model')\n",
        "    print('#' * 20)\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(query, {'success'})\n",
        "\n",
        "    # print(json.dumps(evaluator.evaluate(result_eval), indent=1))\n",
        "    eval = evaluator.evaluate(result_eval)\n",
        "\n",
        "    for measure in sorted(list(eval[list(eval.keys())[0]].keys())):\n",
        "        print(measure, 'average:',\n",
        "              pytrec_eval.compute_aggregated_measure(\n",
        "                  measure, [query_measures[measure] for query_measures in eval.values()])\n",
        "              )\n",
        "    print('#' * 20)\n",
        "# endregion\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "N9ZvWNUNKKuY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
